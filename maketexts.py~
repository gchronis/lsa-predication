import nltk
import gensim
import codecs
from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords, gutenberg

#sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')

#fileids = []
#for id in nltk.corpus.gutenberg.fileids():
#   fileids.append(id)

#texts = codecs.open('gutenberg_lines.txt','w',encoding='utf-8')
#sentences = []
#for f in fileids:
#   for sentence in nltk.corpus.gutenberg.sents(f):
#       sentences.append(sentence)

#for sentence in sentences:
#   texts.write(sentence)



all_docs = []

for f in gutenberg.fileids():
   for sent in gutenberg.sents(f):
      all_docs.append(sent)

stoplist = stopwords.words('english')
texts = [[word for word in document.lower().split() if word not in stoplist]
         for document in documents]
   
all_tokens = sum(texts, [])
tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)
texts = [[word for word in text if word not in tokens_once] 
         for text in texts]

print(texts[1:100])
