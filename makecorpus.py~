import nltk, re, pprint, string
from gensim import corpora, models, similarities
import codecs
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords, gutenberg, PlaintextCorpusReader

#sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')

#fileids = []
#for id in nltk.corpus.gutenberg.fileids():
#   fileids.append(id)

#texts = codecs.open('gutenberg_lines.txt','w',encoding='utf-8')
#sentences = []
#for f in fileids:
#   for sentence in nltk.corpus.gutenberg.sents(f):
#       sentences.append(sentence)

#for sentence in sentences:
#   texts.write(sentence)


gutenberg_dir = nltk.data.find('corpora/gutenberg.zip').join('gutenberg/')
root = '/usr/local/share/nltk_data/corpora/gutenberg/'
reader = PlaintextCorpusReader(gutenberg_dir, '.*\emma.txt') # doctest: +SKIP  #actual regexp should read '.*\.txt'


documents = []

for f in reader.fileids():
   for sent in reader.sents(f):
      documents.append(sent)
      #print(sent)

print(documents)[1:100]

stoplist = stopwords.words('english')
punctuation = string.punctuation+"''"
print(punctuation)


texts = [[word.lower() for word in document if (word not in stoplist and not re.search("\W.*", word))]
         for document in documents]
print(texts[1:100])
   
all_tokens = sum(texts, [])
tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)
texts = [[word for word in text if word not in tokens_once] 
         for text in texts]

print(texts[1:100])

dictionary = corpora.Dictionary(texts)
dictionary.save('/tmp/emma.dict') # store the dictionary, for future reference

print(dictionary)
print(dictionary.token2id)


corpus = [dictionary.doc2bow(text) for text in texts]
corpora.MmCorpus.serialize('/tmp/emma.mm', corpus) # store to disk, for later use
